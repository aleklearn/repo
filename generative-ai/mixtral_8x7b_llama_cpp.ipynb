{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1luw_yN8Wm9mfVhmQW79c8sQs42WxXKwW",
      "authorship_tag": "ABX9TyNC0XPFSolPA3VLnZnd63Qk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aleklearn/repo/blob/main/generative-ai/mixtral_8x7b_llama_cpp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "- [Running Mixtral 8x7b On Google Colab For Free](https://www.kdnuggets.com/running-mixtral-8x7b-on-google-colab-for-free)\n",
        "- [Mixtral of Experts](https://mistral.ai/news/mixtral-of-experts/), [paper](https://arxiv.org/pdf/2401.04088.pdf)\n",
        "- [LLaMA C++](https://github.com/ggerganov/llama.cpp)\n"
      ],
      "metadata": {
        "id": "uKepdhVMhElo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install `llama.cpp`"
      ],
      "metadata": {
        "id": "KwBgPXP1nsel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --depth 1 https://github.com/ggerganov/llama.cpp.git"
      ],
      "metadata": {
        "id": "gOR7tYhvwfDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66476134-0e26-4ca0-a2ef-208c0beaa779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'llama.cpp' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd llama.cpp\n",
        "\n",
        "!make LLAMA_CUBLAS=1"
      ],
      "metadata": {
        "id": "jLQFS_V5wmfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6ca384e-4fe4-497e-8faa-b9d847f67f5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama.cpp\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
            "I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi\n",
            "I NVCCFLAGS: -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 \n",
            "I LDFLAGS:   -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "make: Nothing to be done for 'default'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mixtral 8x7B Model\n",
        "\n",
        "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/blob/main/mixtral-8x7b-instruct-v0.1.Q2_K.gguf\n"
      ],
      "metadata": {
        "id": "QWEoMLsEpMK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq huggingface-hub"
      ],
      "metadata": {
        "id": "WwiYBIYaxFui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF mixtral-8x7b-instruct-v0.1.Q2_K.gguf --local-dir . --local-dir-use-symlinks False"
      ],
      "metadata": {
        "id": "cM12TH1txWR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d013522f-cae5-4aec-b24b-2ee119d5984e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "downloading https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q2_K.gguf to /root/.cache/huggingface/hub/tmplnqswpvi\n",
            "mixtral-8x7b-instruct-v0.1.Q2_K.gguf: 100% 15.6G/15.6G [02:03<00:00, 126MB/s]\n",
            "./mixtral-8x7b-instruct-v0.1.Q2_K.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running the Server"
      ],
      "metadata": {
        "id": "rrb3wIGo4UZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.output import eval_js\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(6589)\"))"
      ],
      "metadata": {
        "id": "lwAXQxGryePS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "590557b6-ac25-4da3-c63f-c82e905f391d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://nb0vw35rmp-496ff2e9c6d22116-6589-colab.googleusercontent.com/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `-m FNAME`, `--model FNAME`: Specify the path to the LLaMA model file (e.g., `models/7B/ggml-model.gguf`).\n",
        "- `-ngl N`, `--n-gpu-layers N`: When compiled with appropriate support (currently CLBlast or cuBLAS), this option allows offloading some layers to the GPU for computation. Generally results in increased performance.\n",
        "- `-c N`, `--ctx-size N`: Set the size of the prompt context. The default is 512, but LLaMA models were built with a context of 2048, which will provide better results for longer input/inference. The size may differ in other models, for example, baichuan models were build with a context of 4096.\n",
        "- `--port`: Set the port to listen. Default: `8080`.\n",
        "\n",
        "[More details](https://github.com/ggerganov/llama.cpp/tree/master/examples/server)"
      ],
      "metadata": {
        "id": "7OsSCJ5Ae5gw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/llama.cpp\n",
        "\n",
        "!./server -m mixtral-8x7b-instruct-v0.1.Q2_K.gguf -ngl 27 -c 2048 --port 6589"
      ],
      "metadata": {
        "id": "vux7RXe0zTKB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "836083c3-4b0f-40ce-deb0-5fd7758102d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama.cpp\n",
            "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_init_cublas: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "{\"timestamp\":1705463346,\"level\":\"INFO\",\"function\":\"main\",\"line\":2864,\"message\":\"build info\",\"build\":1,\"commit\":\"5c99960\"}\n",
            "{\"timestamp\":1705463346,\"level\":\"INFO\",\"function\":\"main\",\"line\":2867,\"message\":\"system info\",\"n_threads\":2,\"n_threads_batch\":-1,\"total_threads\":2,\"system_info\":\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \"}\n",
            "\n",
            "llama server listening at http://127.0.0.1:6589\n",
            "\n",
            "{\"timestamp\":1705463346,\"level\":\"INFO\",\"function\":\"main\",\"line\":2971,\"message\":\"HTTP server listening\",\"port\":\"6589\",\"hostname\":\"127.0.0.1\"}\n",
            "llama_model_loader: loaded meta data with 26 key-value pairs and 995 tensors from mixtral-8x7b-instruct-v0.1.Q2_K.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = mistralai_mixtral-8x7b-instruct-v0.1\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:                         llama.expert_count u32              = 8\n",
            "llama_model_loader: - kv  10:                    llama.expert_used_count u32              = 2\n",
            "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  13:                          general.file_type u32              = 10\n",
            "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type  f16:   32 tensors\n",
            "llama_model_loader: - type q8_0:   64 tensors\n",
            "llama_model_loader: - type q2_K:  801 tensors\n",
            "llama_model_loader: - type q3_K:   32 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 8\n",
            "llm_load_print_meta: n_expert_used    = 2\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 1000000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
            "llm_load_print_meta: model params     = 46.70 B\n",
            "llm_load_print_meta: model size       = 14.57 GiB (2.68 BPW) \n",
            "llm_load_print_meta: general.name     = mistralai_mixtral-8x7b-instruct-v0.1\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.76 MiB\n",
            "{\"timestamp\":1705463410,\"level\":\"INFO\",\"function\":\"log_server_request\",\"line\":2805,\"message\":\"request\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":46362,\"status\":200,\"method\":\"GET\",\"path\":\"/\",\"params\":{}}\n",
            "{\"timestamp\":1705463411,\"level\":\"INFO\",\"function\":\"log_server_request\",\"line\":2805,\"message\":\"request\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":46368,\"status\":200,\"method\":\"GET\",\"path\":\"/index.js\",\"params\":{}}\n",
            "{\"timestamp\":1705463411,\"level\":\"INFO\",\"function\":\"log_server_request\",\"line\":2805,\"message\":\"request\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":46370,\"status\":200,\"method\":\"GET\",\"path\":\"/json-schema-to-grammar.mjs\",\"params\":{}}\n",
            "{\"timestamp\":1705463411,\"level\":\"INFO\",\"function\":\"log_server_request\",\"line\":2805,\"message\":\"request\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":46374,\"status\":200,\"method\":\"GET\",\"path\":\"/completion.js\",\"params\":{}}\n",
            "{\"timestamp\":1705463412,\"level\":\"INFO\",\"function\":\"log_server_request\",\"line\":2805,\"message\":\"request\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":46376,\"status\":404,\"method\":\"GET\",\"path\":\"/favicon.ico\",\"params\":{}}\n",
            "llm_load_tensors: offloading 27 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 27/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size = 14918.57 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size = 12466.41 MiB\n",
            "....................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 2048\n",
            "llama_new_context_with_model: freq_base  = 1000000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:  CUDA_Host KV buffer size =    40.00 MiB\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =   216.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 5\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   192.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =   184.04 MiB\n",
            "Available slots:\n",
            " -> Slot 0 - max context: 2048\n",
            "{\"timestamp\":1705463452,\"level\":\"INFO\",\"function\":\"main\",\"line\":2992,\"message\":\"model loaded\"}\n",
            "all slots are idle and system prompt is empty, clear the KV cache\n",
            "slot 0 is processing [task id: 0]\n",
            "slot 0 : in cache: 0 tokens | to process: 66 tokens\n",
            "slot 0 : kv cache rm - [0, end)\n",
            "\n",
            "print_timings: prompt eval time =    8510.65 ms /    66 tokens (  128.95 ms per token,     7.75 tokens per second)\n",
            "print_timings:        eval time =    2136.62 ms /    17 runs   (  125.68 ms per token,     7.96 tokens per second)\n",
            "print_timings:       total time =   10647.28 ms\n",
            "slot 0 released (83 tokens in cache)\n",
            "{\"timestamp\":1705463593,\"level\":\"INFO\",\"function\":\"log_server_request\",\"line\":2805,\"message\":\"request\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":42876,\"status\":200,\"method\":\"POST\",\"path\":\"/completion\",\"params\":{}}\n",
            "slot 0 released (83 tokens in cache)\n",
            "slot 0 is processing [task id: 2]\n",
            "slot 0 : in cache: 79 tokens | to process: 21 tokens\n",
            "slot 0 : kv cache rm - [79, end)\n",
            "\n",
            "print_timings: prompt eval time =    2292.55 ms /    21 tokens (  109.17 ms per token,     9.16 tokens per second)\n",
            "print_timings:        eval time =    8613.19 ms /    45 runs   (  191.40 ms per token,     5.22 tokens per second)\n",
            "print_timings:       total time =   10905.74 ms\n",
            "slot 0 released (145 tokens in cache)\n",
            "{\"timestamp\":1705463620,\"level\":\"INFO\",\"function\":\"log_server_request\",\"line\":2805,\"message\":\"request\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":37102,\"status\":200,\"method\":\"POST\",\"path\":\"/completion\",\"params\":{}}\n",
            "slot 0 released (145 tokens in cache)\n",
            "slot 0 is processing [task id: 4]\n",
            "slot 0 : in cache: 141 tokens | to process: 20 tokens\n",
            "slot 0 : kv cache rm - [141, end)\n",
            "\n",
            "print_timings: prompt eval time =    2127.77 ms /    20 tokens (  106.39 ms per token,     9.40 tokens per second)\n",
            "print_timings:        eval time =   11505.24 ms /    63 runs   (  182.62 ms per token,     5.48 tokens per second)\n",
            "print_timings:       total time =   13633.01 ms\n",
            "slot 0 released (224 tokens in cache)\n",
            "{\"timestamp\":1705463647,\"level\":\"INFO\",\"function\":\"log_server_request\",\"line\":2805,\"message\":\"request\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":45400,\"status\":200,\"method\":\"POST\",\"path\":\"/completion\",\"params\":{}}\n",
            "slot 0 released (224 tokens in cache)\n",
            "slot 0 is processing [task id: 6]\n",
            "slot 0 : in cache: 220 tokens | to process: 36 tokens\n",
            "slot 0 : kv cache rm - [220, end)\n",
            "\n",
            "print_timings: prompt eval time =    1724.98 ms /    36 tokens (   47.92 ms per token,    20.87 tokens per second)\n",
            "print_timings:        eval time =    8113.55 ms /    41 runs   (  197.89 ms per token,     5.05 tokens per second)\n",
            "print_timings:       total time =    9838.53 ms\n",
            "slot 0 released (297 tokens in cache)\n",
            "{\"timestamp\":1705463681,\"level\":\"INFO\",\"function\":\"log_server_request\",\"line\":2805,\"message\":\"request\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":56242,\"status\":200,\"method\":\"POST\",\"path\":\"/completion\",\"params\":{}}\n",
            "slot 0 released (297 tokens in cache)\n",
            "slot 0 is processing [task id: 8]\n",
            "slot 0 : in cache: 296 tokens | to process: 53 tokens\n",
            "slot 0 : kv cache rm - [296, end)\n",
            "\n",
            "print_timings: prompt eval time =    1925.77 ms /    53 tokens (   36.34 ms per token,    27.52 tokens per second)\n",
            "print_timings:        eval time =    6499.90 ms /    46 runs   (  141.30 ms per token,     7.08 tokens per second)\n",
            "print_timings:       total time =    8425.67 ms\n",
            "slot 0 released (395 tokens in cache)\n",
            "{\"timestamp\":1705463721,\"level\":\"INFO\",\"function\":\"log_server_request\",\"line\":2805,\"message\":\"request\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":58232,\"status\":200,\"method\":\"POST\",\"path\":\"/completion\",\"params\":{}}\n",
            "slot 0 released (395 tokens in cache)\n",
            "slot 0 is processing [task id: 10]\n",
            "slot 0 : in cache: 394 tokens | to process: 26 tokens\n",
            "slot 0 : kv cache rm - [394, end)\n",
            "\n",
            "print_timings: prompt eval time =    2720.79 ms /    26 tokens (  104.65 ms per token,     9.56 tokens per second)\n",
            "print_timings:        eval time =   11356.43 ms /    62 runs   (  183.17 ms per token,     5.46 tokens per second)\n",
            "print_timings:       total time =   14077.22 ms\n",
            "slot 0 released (482 tokens in cache)\n",
            "{\"timestamp\":1705463761,\"level\":\"INFO\",\"function\":\"log_server_request\",\"line\":2805,\"message\":\"request\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":45066,\"status\":200,\"method\":\"POST\",\"path\":\"/completion\",\"params\":{}}\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}