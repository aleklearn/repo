{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1PqA_-j-k2RpdbwpKOdbJptHXS8QtI7r9","authorship_tag":"ABX9TyM0+xsDKKxcLfP+A0Ynfw/b"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","# Retrieval-augmented generation (RAG)"],"metadata":{"id":"cmikg9vVLaPo"}},{"cell_type":"markdown","source":["## Dependencies"],"metadata":{"id":"1uanxLj-XigX"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ae4vv-0DKdrB","executionInfo":{"status":"ok","timestamp":1702712994994,"user_tz":-420,"elapsed":53553,"user":{"displayName":"Alek Learn","userId":"02815819682795970617"}},"outputId":"c1f9da9c-dfbe-4c10-e2b9-28b404b2a8f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.9/221.9 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.7/507.7 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.6/105.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.6/72.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","llmx 0.0.15a0 requires cohere, which is not installed.\n","tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install -Uq langchain openai chromadb langchainhub tiktoken"]},{"cell_type":"code","source":["# For Colab, set OPENAI_API_KEY in Secrets then import to environment cariable\n","import os\n","from google.colab import userdata\n","os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"],"metadata":{"id":"P6i6LmMjkYRp","executionInfo":{"status":"ok","timestamp":1702713001913,"user_tz":-420,"elapsed":6928,"user":{"displayName":"Alek Learn","userId":"02815819682795970617"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# If you're running the notebook on your computer, set OPENAI_API_KEY in your .env file\n","# and install python-dotenv and uncomment code below:\n","\n","# !pip install\n","\n","# from dotenv import load_dotenv\n","\n","# load_dotenv()  # take environment variables from .env"],"metadata":{"id":"MyYloRIeklQp","executionInfo":{"status":"ok","timestamp":1702713001914,"user_tz":-420,"elapsed":8,"user":{"displayName":"Alek Learn","userId":"02815819682795970617"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Overview\n","---\n","The pipeline for converting raw unstructured data into a QA chain looks like this:\n","\n","1. `Loading`: First we need to load our data. Use the [LangChain integration hub](https://integrations.langchain.com/) to browse the full set of loaders.\n","2. `Splitting`: [Text splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/) break `Documents` into splits of specified size\n","3. `Storage`: Storage (e.g., often a [vectorstore](https://python.langchain.com/docs/modules/data_connection/vectorstores/)) will house [and often embed](https://www.pinecone.io/learn/vector-embeddings/) the splits\n","4. `Retrieval`: The app retrieves splits from storage (e.g., often [with similar embeddings](https://www.pinecone.io/learn/k-nearest-neighbor/) to the input question)\n","5. `Generation`: An [LLM](https://python.langchain.com/docs/modules/model_io/models/llms/) produces an answer using a prompt that includes the question and the retrieved data\n"],"metadata":{"id":"aRLOoDONOA16"}},{"cell_type":"markdown","source":["## Use case\n","\n","Suppose you have some text documents (PDF, blog, Notion pages, etc.) and want to ask questions related to the contents of those documents.\n","\n","Let's say we want a QA app over this blog"],"metadata":{"id":"X1ddPAMILKoL"}},{"cell_type":"markdown","source":["## Step 1. Document Loading\n","\n","Specify a `DocumentLoader` to load in your unstructured data as `Documents`.\n","\n","A `Document` is a dict with text (`page_content`) and `metadata`."],"metadata":{"id":"QMIu2PV8PWD7"}},{"cell_type":"code","source":["from langchain.document_loaders import WebBaseLoader\n","\n","loader = WebBaseLoader(\n","    \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n","    )\n","data = loader.load()"],"metadata":{"id":"zDawz--lLPM9","executionInfo":{"status":"ok","timestamp":1702713003891,"user_tz":-420,"elapsed":1983,"user":{"displayName":"Alek Learn","userId":"02815819682795970617"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Step 2. Splitting\n","\n","Split the `Document` into chunks for embedding and vector storage."],"metadata":{"id":"PDDFcGP7SeuV"}},{"cell_type":"code","source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,\n","                                               chunk_overlap=0)\n","\n","all_splits = text_splitter.split_documents(data)"],"metadata":{"id":"BCie5_-WSfXb","executionInfo":{"status":"ok","timestamp":1702713003892,"user_tz":-420,"elapsed":7,"user":{"displayName":"Alek Learn","userId":"02815819682795970617"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## Step 3. Storage\n","\n","To be able to look up our document splits, we first need to store them where we can later look them up.\n","\n","The most common way to do this is to embed the contents of each document split.\n","\n","We store the embedding and splits in a vectorstore.\n","\n"],"metadata":{"id":"PDhOUtxxS2YX"}},{"cell_type":"code","source":["from langchain.embeddings import OpenAIEmbeddings\n","from langchain.vectorstores import Chroma\n","\n","vectorstore = Chroma.from_documents(documents=all_splits,\n","                                    embedding=OpenAIEmbeddings())"],"metadata":{"id":"iW4owI7hS-W3","executionInfo":{"status":"ok","timestamp":1702713009234,"user_tz":-420,"elapsed":5349,"user":{"displayName":"Alek Learn","userId":"02815819682795970617"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## Step 4. Retrieval\n","\n","Retrieve relevant splits for any question using [similarity search](https://www.pinecone.io/learn/what-is-similarity-search/).\n","\n","This is simply \"top K\" retrieval where we select documents based on embedding similarity to the query."],"metadata":{"id":"KVd5eW0bTZv4"}},{"cell_type":"code","source":["question = \"What are the approaches to Task Decomposition?\"\n","docs = vectorstore.similarity_search(question)\n","len(docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YTswsL3uTmeo","executionInfo":{"status":"ok","timestamp":1702713009238,"user_tz":-420,"elapsed":15,"user":{"displayName":"Alek Learn","userId":"02815819682795970617"}},"outputId":"bf9b5103-0144-46b7-8dbd-ebdcc5c8c528"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["retriever = vectorstore.as_retriever()\n","retriever"],"metadata":{"id":"cp2Jbs86fR0B","executionInfo":{"status":"ok","timestamp":1702713009238,"user_tz":-420,"elapsed":12,"user":{"displayName":"Alek Learn","userId":"02815819682795970617"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"034b8a4a-6316-43ec-8084-883b16b52b05"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7969f4df68f0>)"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["## Step 5. Generate Output\n","\n","We used a prompt for RAG that is checked into the LangChain prompt hub ([here](https://smith.langchain.com/hub/rlm/rag-prompt))."],"metadata":{"id":"v67sVDfwTy3x"}},{"cell_type":"code","source":["# Prompt\n","# https://smith.langchain.com/hub/rlm/rag-prompt\n","\n","from langchain import hub\n","rag_prompt = hub.pull(\"rlm/rag-prompt\")\n","rag_prompt"],"metadata":{"id":"idXSRaZdXF1q","executionInfo":{"status":"ok","timestamp":1702713010493,"user_tz":-420,"elapsed":1264,"user":{"displayName":"Alek Learn","userId":"02815819682795970617"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fc6e5f6f-edb4-41b4-ccbd-0246369f20b2"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["Distill the retrieved documents into an answer using an LLM/Chat model (e.g., `gpt-3.5-turbo`)."],"metadata":{"id":"euWg829oaY-C"}},{"cell_type":"code","source":["# LLM\n","\n","from langchain.chat_models import ChatOpenAI\n","\n","llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"],"metadata":{"id":"VMEtP6fbXLyH","executionInfo":{"status":"ok","timestamp":1702713012710,"user_tz":-420,"elapsed":2221,"user":{"displayName":"Alek Learn","userId":"02815819682795970617"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["We use the [Runnable](https://python.langchain.com/docs/expression_language/interface) protocol to define the chain.\n","\n","Runnable protocol pipes together components in a transparent way."],"metadata":{"id":"nkbWnNTFaiHD"}},{"cell_type":"code","source":["# RAG chain\n","\n","from langchain.schema.runnable import RunnablePassthrough\n","\n","rag_chain = ({\"context\": retriever,\n","             \"question\": RunnablePassthrough()}\n","             | rag_prompt\n","             | llm)"],"metadata":{"id":"2FJ69-pzXOrW","executionInfo":{"status":"ok","timestamp":1702713073630,"user_tz":-420,"elapsed":415,"user":{"displayName":"Alek Learn","userId":"02815819682795970617"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["rag_chain.invoke(\"What is Task Decomposition?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NEcaGURQTyHc","executionInfo":{"status":"ok","timestamp":1702713081449,"user_tz":-420,"elapsed":3823,"user":{"displayName":"Alek Learn","userId":"02815819682795970617"}},"outputId":"d337cebf-626b-44f3-8491-7b9bebe99555"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='Task decomposition is the process of breaking down a large task into smaller, manageable subgoals. It enables efficient handling of complex tasks and improves the quality of final results. However, long-term planning and effective exploration of the solution space remain challenging for language models like LLM.')"]},"metadata":{},"execution_count":14}]}]}